{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-07QJORT.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Year of Birth'),\n",
       " (1, 'Gender'),\n",
       " (2, 'Ethnicity'),\n",
       " (3, \"Child's First Name\"),\n",
       " (4, 'Count'),\n",
       " (5, 'Rank')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0 = sc.textFile(\"Popular_Baby_Names.csv\").cache()\n",
    "list(enumerate(rdd0.first().split(',')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Map and FlatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " ('2016', '172'),\n",
       " ('2016', '112'),\n",
       " ('2016', '104'),\n",
       " ('2016', '99'),\n",
       " ('2016', '99'),\n",
       " ('2016', '79'),\n",
       " ('2016', '59'),\n",
       " ('2016', '57'),\n",
       " ('2016', '56')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Yields a None for the header to skip use MapPartitions with index.\n",
    "# Diff from flatMap - flattens the output.\n",
    "def func1(row):\n",
    "    arr = row.split(\",\")\n",
    "    if (arr[0] != 'Year of Birth' and arr[4] != 'Count'):\n",
    "        return (arr[0],arr[4])\n",
    "rdd0.map(func1).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) mapPartitionsWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2016', '172'),\n",
       " ('2016', '112'),\n",
       " ('2016', '104'),\n",
       " ('2016', '99'),\n",
       " ('2016', '99'),\n",
       " ('2016', '79'),\n",
       " ('2016', '59'),\n",
       " ('2016', '57'),\n",
       " ('2016', '56'),\n",
       " ('2016', '56')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func1(index,rows):\n",
    "    if index==0:\n",
    "        next(rows)\n",
    "    import csv \n",
    "    reader = csv.reader(rows)\n",
    "    for field in reader:\n",
    "        yield (field[0],field[4])\n",
    "rdd0.mapPartitionsWithIndex(func1).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2016', ('Olivia', '1')),\n",
       " ('2016', ('Ava', '1')),\n",
       " ('2016', ('Isabella', '1')),\n",
       " ('2016', ('Olivia', '1')),\n",
       " ('2016', ('Ethan', '1')),\n",
       " ('2016', ('Noah', '1')),\n",
       " ('2016', ('Liam', '1')),\n",
       " ('2016', ('Joseph', '1')),\n",
       " ('2015', ('Olivia', '1')),\n",
       " ('2015', ('Madison', '1')),\n",
       " ('2015', ('Isabella', '1')),\n",
       " ('2015', ('Emma', '1')),\n",
       " ('2015', ('Olivia', '1')),\n",
       " ('2015', ('Jayden', '1')),\n",
       " ('2015', ('Noah', '1')),\n",
       " ('2015', ('Liam', '1')),\n",
       " ('2015', ('David', '1')),\n",
       " ('2014', ('Liam', '1')),\n",
       " ('2013', ('Olivia', '1')),\n",
       " ('2013', ('David', '1')),\n",
       " ('2012', ('CHLOE', '1')),\n",
       " ('2012', ('EMMA', '1')),\n",
       " ('2012', ('RYAN', '1')),\n",
       " ('2012', ('JOSEPH', '1')),\n",
       " ('2011', ('SOPHIA', '1')),\n",
       " ('2011', ('MADISON', '1')),\n",
       " ('2011', ('ISABELLA', '1')),\n",
       " ('2011', ('ESTHER', '1')),\n",
       " ('2011', ('ETHAN', '1')),\n",
       " ('2011', ('JAYDEN', '1')),\n",
       " ('2011', ('MICHAEL', '1'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get the rank 1 names for all the years\n",
    "def func1(index,rows):\n",
    "    if index == 0:\n",
    "        next(rows)\n",
    "    import csv\n",
    "    reader = csv.reader(rows)\n",
    "    for fields in reader :\n",
    "        yield (fields[0],(fields[3],fields[5]))\n",
    "rdd0.mapPartitionsWithIndex(func1).filter(lambda x :x[1][1] == '1').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) GroupBy\n",
    "The “groupBy”  transformation will group the data in the original RDD. It creates a set of key value pairs, where the key is output of a user function, and the value is all items for which the function yields this key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Group names by first three letters\n",
    "def func1(index,rows):\n",
    "    if index == 0:\n",
    "        next(rows)\n",
    "    import csv\n",
    "    reader = csv.reader(rows)\n",
    "    for fields in reader :\n",
    "        yield fields[3]\n",
    "rdd1= rdd0.mapPartitionsWithIndex(func1).groupBy(lambda x :x[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('EIT', ['EITAN', 'EITAN']), ('LEE', ['LEELA']), ('GUS', ['GUSTAVO', 'GUSTAVO']), ('KEI', ['KEIRA', 'KEITH', 'KEILY', 'KEITH']), ('MAS', ['MASON', 'MASON', 'MASON', 'MASON', 'MASON', 'MASON', 'MASON', 'MASON']), ('Das', ['Dashiell']), ('KIE', ['KIERAN', 'KIERAN']), ('Ada', ['Ada', 'Adam', 'Adam', 'Adam', 'Adam', 'Ada', 'Adam', 'Adam', 'Adam', 'Adam', 'Ada', 'Adam', 'Adam', 'Adan', 'Adam', 'Ada', 'Ada', 'Adam', 'Adam', 'Adan', 'Adam']), ('HAD', ['HADLEY', 'HADASSAH', 'HADASSAH', 'HADASSA']), ('Cad', ['Caden', 'Caden', 'Caden'])]\n"
     ]
    }
   ],
   "source": [
    "print([(k, list(v)) for (k, v) in rdd1.take(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) ReduceByKey/GroupByKey \n",
    "ReduceByKey has less shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the average count for a name over all the years \n",
    "def func1(index,rows):\n",
    "    if index == 0:\n",
    "        next(rows)\n",
    "    import csv\n",
    "    reader = csv.reader(rows)\n",
    "    for fields in reader :\n",
    "        yield (fields[3],int(fields[4]))\n",
    "def func2(values):\n",
    "    return sum(values)/len(values)\n",
    "rdd1= rdd0.mapPartitionsWithIndex(func1).groupByKey().mapValues(func2).map(lambda x: (x[1],x[0])).sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(251.66666666666666, 'Moshe'),\n",
       " (231.0, 'MOSHE'),\n",
       " (178.36363636363637, 'Ethan'),\n",
       " (177.25, 'Chaya'),\n",
       " (176.0, 'CHAYA'),\n",
       " (174.5, 'CAMILA'),\n",
       " (165.33333333333334, 'Miriam'),\n",
       " (163.14285714285714, 'ETHAN'),\n",
       " (159.07142857142858, 'Liam'),\n",
       " (158.0, 'CHAIM')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(index,rows):\n",
    "    if index == 0:\n",
    "        next(rows)\n",
    "    import csv\n",
    "    reader = csv.reader(rows)\n",
    "    for fields in reader :\n",
    "        yield (fields[3],(int(fields[4]),1))\n",
    "rdd1= rdd0.mapPartitionsWithIndex(func1).reduceByKey(lambda x,y:(x[0]+y[0], x[1]+y[1])).mapValues(lambda x: x[0]/x[1]).map(lambda x: (x[1],x[0])).sortByKey(False).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(251.66666666666666, 'Moshe'),\n",
       " (231.0, 'MOSHE'),\n",
       " (178.36363636363637, 'Ethan'),\n",
       " (177.25, 'Chaya'),\n",
       " (176.0, 'CHAYA'),\n",
       " (174.5, 'CAMILA'),\n",
       " (165.33333333333334, 'Miriam'),\n",
       " (163.14285714285714, 'ETHAN'),\n",
       " (159.07142857142858, 'Liam'),\n",
       " (158.0, 'CHAIM')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10266"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd0.mapPartitionsWithIndex(func1).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)Action: Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize(range(1,100))\n",
    "num_rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Min and Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(index,rows):\n",
    "    if index == 0:\n",
    "        next(rows)\n",
    "    import csv\n",
    "    reader = csv.reader(rows)\n",
    "    for fields in reader :\n",
    "        yield (int(fields[5]))\n",
    "rdd1= rdd0.mapPartitionsWithIndex(func1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+--------------------+------------------+-----+----+\n",
      "|Year of Birth|Gender|           Ethnicity|Child's First Name|Count|Rank|\n",
      "+-------------+------+--------------------+------------------+-----+----+\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Olivia|  172|   1|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|             Chloe|  112|   2|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Sophia|  104|   3|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|             Emily|   99|   4|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|              Emma|   99|   4|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|               Mia|   79|   5|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|         Charlotte|   59|   6|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|             Sarah|   57|   7|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Hannah|   56|   8|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|          Isabella|   56|   8|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|             Grace|   54|   9|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Angela|   54|   9|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|               Ava|   53|  10|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Joanna|   49|  11|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Amelia|   44|  12|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Evelyn|   42|  13|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|              Arya|   42|  13|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|              Ella|   42|  13|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|            Ariana|   40|  14|\n",
      "|         2016|FEMALE|ASIAN AND PACIFIC...|             Alina|   39|  15|\n",
      "+-------------+------+--------------------+------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load('Popular_Baby_Names.csv', format='csv', \n",
    "                           header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
